{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e742cfb",
   "metadata": {},
   "source": [
    "# Tests unsuitable for unittest\n",
    "\n",
    "Some of the functions included in the `gilbert_elliot_model` package are not well suited to be covered by `unittest` tests as they are both slow and involve many random processes. The randomness requires running many repetitions and considering the distribution of results, but the slowness causes this to be too inefficient to do in the `unittest` format. Thus this document analyzes results that were obtained via `distribution_tests.py` so that we can consider the distributions of results to characterize how parts of the package are working.\n",
    "\n",
    "The code chunk below prepares us for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2d43bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import gilbert_elliot_model as ge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2db923",
   "metadata": {},
   "source": [
    "\n",
    "## Fitting a Hidden Markov Model\n",
    "\n",
    "The `gilbert_elliot_model` package is designed to take an observed error pattern and fit the model to it. The Gilbert-Elliot burst error model is a two-state hidden Markov model. Thus the package `hmmlearn` is primarily used for this fitting. However the function `gilbert_elliot_model.gilbert_elliot.fit_hmm` functions as a wrapper for `hmmlearn` functionality that is specifically tuned to the Gilbert-Elliot burst error model. \n",
    "\n",
    "The documentation for the function `gilbert_elliot_model.gilbert_elliot.fit_hmm` is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c2cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ge.gilbert_elliot.fit_hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8047bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_param_fname = 'Fixed_Parameter_README.md'\n",
    "fixed_param_vals = pd.read_csv(fixed_param_fname, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_k_h(df, stats, fixed_param_vals):\n",
    "    for stat in stats:\n",
    "        stat_err = stat + '_error'\n",
    "        if stat_err not in df.columns:\n",
    "            if stat == 'k':\n",
    "                df[stat_err] = df[stat] - 1\n",
    "            elif stat == 'h':\n",
    "                 df[stat_err] = df[stat]  \n",
    "        print(f'All values of {stat} are correct: {all(df[stat_err] == 0)}')\n",
    "def error_histograms(df, stats, fixed_param_vals=None):\n",
    "    for stat in stats:\n",
    "        stat_err = stat + '_error'\n",
    "        if stat_err not in df.columns:\n",
    "            df[stat_err] = df[stat] - fixed_param_vals.loc[0, stat]\n",
    "\n",
    "        stat_mean = np.mean(df[stat_err])\n",
    "        stat_unc = np.std(df[stat_err])/np.sqrt(df.shape[0])\n",
    "        stat_CI = stat_mean + np.array([-1 ,1]) * 1.96 * stat_unc\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Histogram(x=df[stat_err]))\n",
    "        fig.add_vline(x=0, line_dash='dot', line_color = 'red',\n",
    "                     annotation_text='Success criteria', annotation_position='bottom left',\n",
    "                     )\n",
    "        fig.add_vline(x=stat_CI[0], line_dash='dash', line_color='black',)\n",
    "        fig.add_vline(x=stat_CI[1], line_dash='dash', line_color='black',\n",
    "                     annotation_text='95% Confidence Interval', annotation_position='top right'\n",
    "                     )\n",
    "        if fixed_param_vals is not None:\n",
    "            title_str = f'{stat} error from expected ({fixed_param_vals.loc[0, stat]})'\n",
    "        else:\n",
    "            title_str = f'{stat} error'\n",
    "        fig.update_layout(\n",
    "            title={'text': title_str}\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(title='Error')\n",
    "        fig.show(renderer='notebook')\n",
    "\n",
    "def update_df_error_stats(df, expected_stats=None, stats=None):\n",
    "    if expected_stats is None:\n",
    "        est_flag = '_estimate'\n",
    "        target_flag = '_target'\n",
    "        expected_stats = {}\n",
    "        for stat in stats:\n",
    "            expected_stats[stat]= None\n",
    "    else:\n",
    "        est_flag = ''\n",
    "    for out_stat, value in expected_stats.items():\n",
    "        df[out_stat] = df.apply(\n",
    "        lambda row : ge.model_error_statistics(p=row['p' + est_flag],\n",
    "                                               r=row['r' + est_flag],\n",
    "                                               k=row['k' + est_flag],\n",
    "                                               h=row['h' + est_flag])[out_stat],axis=1,)\n",
    "        expected_out_stat = 'expected_' + out_stat\n",
    "        if value is None:\n",
    "            df[expected_out_stat] = df.apply(\n",
    "                lambda row : ge.model_error_statistics(p=row['p' + target_flag],\n",
    "                                                       r=row['r' + target_flag],\n",
    "                                                       k=row['k' + target_flag],\n",
    "                                                       h=row['h' + target_flag])[out_stat],axis=1,)\n",
    "        else:\n",
    "            df[expected_out_stat] = value\n",
    "        diff_out_stat = out_stat + '_error'\n",
    "        df[diff_out_stat] = df[expected_out_stat] - df[out_stat]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd303a6c",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "\n",
    "Two types of tests were performed. One with fixed parameter values for 1000 repetitions and one with random parameter values for each repetition. For the two-parameter model, `k=1, h=0` were always fixed, for the three-parameter model `k=1` was always fixed. Otherwise, in fixed parameter tests, the parameters were set to the values below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bd002",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fixed_param_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62bb004",
   "metadata": {},
   "source": [
    "For both test types we will consider distributions of model parameter estimations directly. However, as we are fitting a hidden Markov model we expect that there will be many combinations of model parameters that would emit similar error patterns. This means it is likely we will see discrepancies from expectation when directly considering model parameters. \n",
    "\n",
    "To alleviate this we will also consider the distributions of error statistics derived from model parameter estimates.\n",
    "\n",
    "### Fixed values tests\n",
    "In a fixed value test we fix some subset of `(p, r, k, h)` depending on which version of the Gilbert-Elliot model we are testing. We then repeat the following $N_{iter}$ times: \n",
    "* Generate error pattern from model parameters of length $N_{err}$.\n",
    "* Estimate the corresponding parameters $(\\hat{p}, \\hat{r}, \\hat{k}, \\hat{h})$\n",
    "* Calculate the error statistics from the estimates, e.g., $\\hat{\\bar{x}}(\\hat{p}, \\hat{r}, \\hat{k}, \\hat{h})$.\n",
    "\n",
    "We can then compare the distribution of differences between error statistic estimates and expected error statistics. Here the expected error statistic is calculated from the true model parameters, e.g., $\\bar{x}(p, r, k, h)$. For example we consider the distribution of error rate errors via $\\bar{x} - \\hat{\\bar{x}}$.\n",
    "\n",
    "The diagram below demonstrates this methodology.\n",
    "![Fixed value test diagram](./figs/fixed_value_test_diagram.png)\n",
    "\n",
    "### Random values tests\n",
    "In a random value test the procedure is almost identical except we generate a new set of $(p, r, k, h)$ each iteration and also calculate the relevant true error statistics each iteration. This process is shown in the diagram below.\n",
    "![Random value test diagram](./figs/random_value_test_diagram.png)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "We expect to see errors in the direct estimation of parameters for the three- and four- parameter models. This is because in these models we are dealing with a true hidden markov model, meaning the model parameters are not directly observable. However we expect that all error statistics that are directly observable, namely the error rate, $\\bar{x}$ and expected burst length, $L_1$, will be well estimated and the observed errors in the model parameter estimations will be driven by error statstics we cannot directly observe such as the proportion of time spent in the bad state $\\pi_B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90951a94",
   "metadata": {},
   "source": [
    "## Results\n",
    "For all these tests `N_iter=1000` and `N_err=1000`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f403f5b",
   "metadata": {},
   "source": [
    "### Two-parameter fixed values\n",
    "\n",
    "For the two-parameter fixed values test the distributions of both `p` and `r` match expectations. Upon inspection the 95% confidence interval for `p` barely does not contain 0, but the distribution is centered near 0 and the discrepancy is very small and does not indicate anything majorly wrong happening with the software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'two_param_distributions_fixed_values.csv'\n",
    "df_2pf = pd.read_csv(fname)\n",
    "\n",
    "# Confirm that all k values 1, all h values 0\n",
    "check_k_h(df_2pf, stats=['k', 'h'], fixed_param_vals=fixed_param_vals)\n",
    "\n",
    "stats = ['p', 'r']\n",
    "error_histograms(df_2pf, stats=stats, fixed_param_vals=fixed_param_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478fd4cf",
   "metadata": {},
   "source": [
    "### Two-parameter random values\n",
    "\n",
    "For the test where `p` and `r` are randomly generated each trial we see even better agreement and the distributions match expectations exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'two_param_distributions_random_errors.csv'\n",
    "df_2pr = pd.read_csv(fname)\n",
    "# Confirm that all k values 1, all h values 0\n",
    "check_k_h(df_2pr, stats=['k', 'h'], fixed_param_vals=fixed_param_vals)\n",
    "\n",
    "stats = ['p', 'r']\n",
    "error_histograms(df_2pr, stats=stats,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b602c",
   "metadata": {},
   "source": [
    "### Three-parameter fixed\n",
    "\n",
    "When we move to the three-parameter case we can pretty quickly see that we do not get great agreement between the parameter estimates and truth. The histograms of errors for model parameters below shows this pretty well. As explained above, this doesn't mean that the fitting process is completely flawed, it seems natural that a variety of parameter pairs `(p, r, h)` could yield similar error pattern results and thus be difficult to distinguish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e97c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'three_param_distributions_fixed_values.csv'\n",
    "df_3pf = pd.read_csv(fname)\n",
    "# Confirm that all k values 1, all h values 0\n",
    "check_k_h(df_3pf, stats=['k'], fixed_param_vals=fixed_param_vals)\n",
    "\n",
    "stats = ['p', 'r', 'h']\n",
    "error_histograms(df_3pf, stats=stats, fixed_param_vals=fixed_param_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4926d",
   "metadata": {},
   "source": [
    "In order to account for this discrepancy we instead consider the errors from the error statistics generated by the original model parameters compared to those computed by the estimate parameters. The plots below show that for the error statistics that are direclty observable, namely error rate and expected burst length, we estimate those well. However the proportion of time spent in the bad state is not directly observable and contributes to the erroneous estimates for `(p, r, h)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_stats = ge.model_error_statistics(p=fixed_param_vals.loc[0, 'p'],\n",
    "                                           r=fixed_param_vals.loc[0, 'r'],\n",
    "                                           h=fixed_param_vals.loc[0, 'h'],\n",
    "                                           k=1,\n",
    "                                           )\n",
    "df_3pf = update_df_error_stats(df_3pf, expected_stats=expected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac0528",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_stats = ['error_rate', 'expected_burst_length', 'bad_proportion']\n",
    "error_histograms(df_3pf, stats=error_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71adf4f5",
   "metadata": {},
   "source": [
    "### Aside\n",
    "Note that for all the plots below, the analysis is essentially the same as above. Namely the software correctly identifies model parameters that generate the relevant error statistics but estimation errors occur when estimating non-directly-observable events/error statistics. These errors ultimately propagate into the model parameter estimates causing them to be off from truth.\n",
    "\n",
    "### Three parameter-random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1871b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'three_param_distributions_random_errors.csv'\n",
    "df_3pr = pd.read_csv(fname)\n",
    "# Confirm that all k values 1, all h values 0\n",
    "check_k_h(df_3pr, stats=['k'], fixed_param_vals=fixed_param_vals)\n",
    "\n",
    "stats = ['p', 'r', 'h']\n",
    "error_histograms(df_3pr, stats=stats, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7354c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3pr = update_df_error_stats(df_3pr, stats=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffd1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_histograms(df_3pr, stats=error_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd67203d",
   "metadata": {},
   "source": [
    "### Four parameter-fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d04209",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'four_param_distributions_fixed_values.csv'\n",
    "df_4pf = pd.read_csv(fname)\n",
    "\n",
    "stats = ['p', 'r', 'h', 'k']\n",
    "error_histograms(df_4pf, stats=stats, fixed_param_vals=fixed_param_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca02e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_stats = ge.model_error_statistics(p=fixed_param_vals.loc[0, 'p'],\n",
    "                                           r=fixed_param_vals.loc[0, 'r'],\n",
    "                                           h=fixed_param_vals.loc[0, 'h'],\n",
    "                                           k=fixed_param_vals.loc[0, 'k'],\n",
    "                                           )\n",
    "df_4pf = update_df_error_stats(df_4pf, expected_stats=expected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5941b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_histograms(df_4pf, stats=error_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b00ada",
   "metadata": {},
   "source": [
    "### Four-parameter random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff51337",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'four_param_distributions_random_errors.csv'\n",
    "df_4pr = pd.read_csv(fname)\n",
    "\n",
    "stats = ['p', 'r', 'h', 'k']\n",
    "error_histograms(df_4pr, stats=stats, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d31362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4pr = update_df_error_stats(df_4pr, stats=error_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af78cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_histograms(df_4pr, stats=error_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
