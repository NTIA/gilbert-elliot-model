{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5728e15f",
   "metadata": {},
   "source": [
    "# Testing `gilbert_elliot_model.determine_model`\n",
    "\n",
    "This document builds off of `distribution_analysis.ipynb`, and focuses on the behavior of the package method `determine_model`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import gilbert_elliot_model as ge\n",
    "\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ad1b0",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "For these tests a true model was selected; one of either two-parameter, three-parameter, or four-parameter.\n",
    "For a given model, the following procedure was repeated 300 times:\n",
    "* Generate error pattern of length `n_obs = 1000`, call error pattern `errors`\n",
    "* Run `ge.determine_model(errors)`\n",
    "* Save the true model, the estimated model, and the associated most likely parameters for all possible models that are output from `ge.determine_model(errors)`.\n",
    "\n",
    "This was performed for each of the possible models.\n",
    "\n",
    "It is important to note that for most practical applications, `n_obs = 1000` is very short.\n",
    "It is likely that we would see better identification success if `n_obs` was much larger, but that analysis would have taken longer to run and was not deemed necessary for this document.\n",
    "\n",
    "## Conclusions\n",
    "These conclusions are drawn from results shown in the [`Analysis`](##Analysis) section.\n",
    "\n",
    "This analysis shows that the function `gilbert_elliot_model.determine_model()` tends to over-predict that a four-parameter model is likely. However it shows that relevant error statistics are preserved regardless. It is best practice to use expert judgement when determining which model to use, however this function can still be a worthwhile tool for comparing model options as needed.\n",
    "In particular, this analysis suggests that without a specific reason for using a more complicated model, a simpler one can work very well.\n",
    "It is important to notice as well however that if the error patterns had been longer, it is likely the correct model would have been identified more.\n",
    "This is because as a pattern gets longer, the parameters of the model matter more and will then be easier to to estimate as well.\n",
    "\n",
    "## Analysis\n",
    "The following sections show a series of plots for each model type. \n",
    "Success is defined as, given an observation pattern, `errors` generated by model X, then model X is identified as the most-likely model to have generated `errors`.\n",
    "If any other model is identified as more likely we label that trial a failure.\n",
    "\n",
    "The first plots is a histogram of which model type was identified as most likely, with the success rate given in the title.\n",
    "The second plot shows the estimated model error rate vs the true model error rate.\n",
    "In particular, given the parameters $(p, r, k, h)$ were used to generate an observation, `errors`, and $(\\hat{p}, \\hat{r}, \\hat{k}, \\hat{h})$ were returned as the parameter estimates associated with the most-likely model we compare the true model error rate $\\bar{x}(p, r, k, h)$ with the estimated model error rate $\\hat{\\bar{x}}(\\hat{p}, \\hat{r}, \\hat{k}, \\hat{h})$.\n",
    "The final plot does the same comparison but with the true model burst length, $L_1(p, r, k, h)$ and the estimated model burst length $\\hat{L}_1(\\hat{p}, \\hat{r}, \\hat{k}, \\hat{h})$ \n",
    "\n",
    "The documentation for the function `gilbert_elliot_model.determine_model` is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49df1a75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(ge.determine_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_df_error_stats(df, expected_stats=None, stats=None):\n",
    "    est_flag = '_estimate'\n",
    "    target_flag = '_target'\n",
    "    stats = ['error_rate', 'expected_burst_length']\n",
    "    for out_stat in stats:\n",
    "        df[out_stat] = df.apply(\n",
    "        lambda row : ge.model_error_statistics(p=row['p' + est_flag],\n",
    "                                               r=row['r' + est_flag],\n",
    "                                               k=row['k' + est_flag],\n",
    "                                               h=row['h' + est_flag])[out_stat],axis=1,)\n",
    "        expected_out_stat = 'expected_' + out_stat\n",
    "        df[expected_out_stat] = df.apply(\n",
    "            lambda row : ge.model_error_statistics(p=row['p' + target_flag],\n",
    "                                                   r=row['r' + target_flag],\n",
    "                                                   k=row['k' + target_flag],\n",
    "                                                   h=row['h' + target_flag])[out_stat],axis=1,)\n",
    "        diff_out_stat = out_stat + '_error'\n",
    "        df[diff_out_stat] = df[expected_out_stat] - df[out_stat]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d67b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data\n",
    "models = ['two', 'three', 'four']\n",
    "template_string = 'determine_model_XXX_param.csv'\n",
    "\n",
    "model_dfs = {}\n",
    "for model in models:\n",
    "    fname = template_string.replace('XXX', model)\n",
    "    model_dfs[model] = pd.read_csv(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name, df in model_dfs.items():\n",
    "#     print(f\"{model_name} accuracy: {np.mean(df['model_est'] == df['model_type'])}\")\n",
    "\n",
    "def model_analysis(df):\n",
    "\n",
    "    figures = []\n",
    "\n",
    "    success_ix = df['model_est'] == df['model_type']\n",
    "    id_rate = 100*np.mean(df['model_est'] == df['model_type'])\n",
    "\n",
    "    fig = px.histogram(df, x='model_est',\n",
    "                       title=f'{model_name}-parameter identification rate: {id_rate:.1f}%'\n",
    "                      )\n",
    "    figures.append(fig)\n",
    "\n",
    "    params = ['p', 'r', 'k', 'h']\n",
    "    new_rows = []\n",
    "    for ix, row in df.iterrows():\n",
    "        new_row = {}\n",
    "        new_row['model_target'] = row['model_type']\n",
    "        new_row['model_estimate'] = row['model_est']\n",
    "        new_row['success'] = row['model_type'] == row['model_est']\n",
    "        for pr in params:\n",
    "            target_pr = pr + '_' + row['model_type']\n",
    "            if row['model_est'] != row['model_type']:\n",
    "                select_pr = pr + '_' + row['model_est']\n",
    "            else:\n",
    "                select_pr = target_pr\n",
    "            new_row[pr + '_target'] = row[target_pr]\n",
    "            new_row[pr + '_estimate'] = row[select_pr]\n",
    "        new_rows.append(new_row)\n",
    "    new_df = pd.DataFrame(new_rows)                  \n",
    "    new_df = update_df_error_stats(new_df)\n",
    "\n",
    "\n",
    "\n",
    "    stat = 'error_rate'\n",
    "    stats = ['error_rate', 'expected_burst_length']\n",
    "    for stat in stats:\n",
    "        figi = px.scatter(new_df,\n",
    "                        x=stat,\n",
    "                        y='expected_' + stat,\n",
    "                        color='success',\n",
    "                        title=stat + ' expectation vs estimation',\n",
    "                        )\n",
    "\n",
    "        figures.append(figi)\n",
    "\n",
    "\n",
    "    for fig in figures:\n",
    "        fig.show(renderer='notebook')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085bfa5",
   "metadata": {},
   "source": [
    "### Two-parameter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'two'\n",
    "model_analysis(model_dfs[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4fb5b5",
   "metadata": {},
   "source": [
    "### Three-parameter model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fac9a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'three'\n",
    "model_analysis(model_dfs[model_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7b9f17",
   "metadata": {},
   "source": [
    "### Four-parameter Model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'four'\n",
    "model_analysis(model_dfs[model_name])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
